<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Conceptos básicos - Aprendizaje por Refuerzo</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
  <style>
    .card {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 15px;
      box-shadow: 0px 4px 15px rgba(7, 7, 7, 0.3);
    }
    .accordion-button {
      background: rgba(102, 126, 234, 0.9);
      color: #fff;
      font-weight: bold;
    }
    .accordion-button:not(.collapsed) {
      background: #667eea;
      color: #fff;
    }
    .accordion-body {
      color: #212529;
    }
    .navbar{ background-color: black; }
    .dropdown-menu { background-color: #212529; }
    .dropdown-item { color: #fff; }
    .dropdown-item:hover { background-color: #343a40; }
    .dropdown-submenu{ position: relative }
    .dropdown-submenu > .dropdown-menu{ display:none; top:0; left:100%; margin-top: 3px; }
    .dropdown-submenu:hover > .dropdown-menu { display: block; }
    .referencias {
      background: #f8f9fa;
      padding: 20px;
      border-left: 4px solid #667eea;
      margin-top: 20px;
    }
  </style>
</head>
<body class="bgH1">

<nav class="navbar navbar-expand-lg bg-dark navbar-dark"> 
    <div class="container-fluid"> 
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" 
        aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> 
        <span class="navbar-toggler-icon"></span> 
      </button> 
      <div class="collapse navbar-collapse" id="navbarSupportedContent"> 
        <ul class="navbar-nav me-auto mb-2 mb-lg-0"> 
          <li class="nav-item"> 
            <a class="nav-link active" aria-current="page" href="/">Home</a> 
          </li> 
          <li class="nav-item dropdown"> 
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false"> 
              Menu 
            </a> 
            <ul class="dropdown-menu" aria-labelledby="navbarDropdown"> 
              <li class="dropdown-submenu"> 
                <a class="dropdown-item dropdown-toggle" href="#">Casos ML</a> 
                <ul class="dropdown-menu"> 
                  <li><a class="dropdown-item" href="/casoUno">Caso 1</a></li> 
                  <li><a class="dropdown-item" href="/casoDos">Caso 2</a></li> 
                  <li><a class="dropdown-item" href="/casoTres">Caso 3</a></li> 
                  <li><a class="dropdown-item" href="/casoCuatro">Caso 4</a></li> 
                </ul> 
              </li> 
              <li class="dropdown-submenu">
                <a class="dropdown-item dropdown-toggle" href="#">Regresión Lineal</a>
                <ul class="dropdown-menu">
                  <li><a class="dropdown-item" href="/regresionConceptos">Conceptos</a></li>
                  <li><a class="dropdown-item" href="/LR">Práctico</a></li>
                </ul>
              </li>
              <li class="dropdown-submenu">
                <a class="dropdown-item dropdown-toggle" href="#">Regresión Logística</a>
                <ul class="dropdown-menu">
                  <li><a class="dropdown-item" href="/LogisConceptos">Conceptos</a></li>
                  <li><a class="dropdown-item" href="/Lc">Práctico</a></li>
                </ul>
              </li>
              <li class="dropdown-submenu">
                <a class="dropdown-item dropdown-toggle" href="#">Naive Bayes</a>
                <ul class="dropdown-menu">
                  <li><a class="dropdown-item" href="/naivebayes">Práctico</a></li>
                </ul>
              </li>
              <li class="dropdown-submenu">
                <a class="dropdown-item dropdown-toggle" href="#">Aprendizaje por Refuerzo</a>
                <ul class="dropdown-menu">
                  <li><a class="dropdown-item" href="/RLConceptos">Conceptos Básicos</a></li>
                  <li><a class="dropdown-item" href="/RLPractico">Caso Práctico</a></li>
                </ul>
              </li>
              <li class="dropdown-submenu">
                <a class="dropdown-item dropdown-toggle" href="#">Tipos de Algoritmos de Clasificación</a>
                <ul class="dropdown-menu">
                  <li><a class="dropdown-item" href="/Practicos">Prácticos</a></li>
                </ul>
              </li>
            </ul> 
          </li> 
        </ul> 
      </div>
    </div>
  </nav> 

<div class="container my-5">
  <h2 class="text-center mb-4 text-white">Conceptos Básicos de Aprendizaje por Refuerzo</h2>
  <div class="accordion" id="accordionConceptos">
    <div class="accordion-item card mb-3">
      <h2 class="accordion-header" id="headingOne">
        <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne">
          ¿Qué es el Aprendizaje por Refuerzo?
        </button>
      </h2>
      <div id="collapseOne" class="accordion-collapse collapse show" data-bs-parent="#accordionConceptos">
        <div class="accordion-body">
          <p>El <strong>Aprendizaje por Refuerzo</strong> (Reinforcement Learning, RL) es un paradigma de aprendizaje automático donde un agente aprende a tomar decisiones mediante la interacción con un entorno. El agente recibe recompensas o penalizaciones basadas en las acciones que realiza, con el objetivo de maximizar la recompensa acumulada a largo plazo (Sutton & Barto, 2018).</p>
          <p>A diferencia del aprendizaje supervisado, donde el modelo aprende de ejemplos etiquetados, en RL el agente descubre qué acciones tomar a través de ensayo y error, recibiendo retroalimentación en forma de recompensas.</p>
        </div>
      </div>
    </div>
    <div class="accordion-item card mb-3">
      <h2 class="accordion-header" id="headingTwo">
        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo">
          Componentes Principales
        </button>
      </h2>
      <div id="collapseTwo" class="accordion-collapse collapse" data-bs-parent="#accordionConceptos">
        <div class="accordion-body">
          <ul>
            <li><strong>Agente:</strong> La entidad que toma decisiones y aprende de la experiencia.</li>
            <li><strong>Entorno:</strong> El mundo con el que interactúa el agente.</li>
            <li><strong>Estado (s):</strong> Una representación de la situación actual del entorno.</li>
            <li><strong>Acción (a):</strong> Las posibles decisiones que puede tomar el agente.</li>
            <li><strong>Recompensa (r):</strong> Señal numérica que indica qué tan buena fue una acción.</li>
            <li><strong>Política (π):</strong> Estrategia que define qué acción tomar en cada estado.</li>
            <li><strong>Función de Valor:</strong> Estima la recompensa esperada a largo plazo desde un estado dado.</li>
          </ul>
        </div>
      </div>
    </div>
    <div class="accordion-item card mb-3">
      <h2 class="accordion-header" id="headingThree">
        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree">
          Q-Learning
        </button>
      </h2>
      <div id="collapseThree" class="accordion-collapse collapse" data-bs-parent="#accordionConceptos">
        <div class="accordion-body">
          <p><strong>Q-Learning</strong> es uno de los algoritmos fundamentales en RL. Utiliza una tabla Q(s,a) que estima el valor de tomar una acción 'a' en un estado 's'. La actualización se realiza mediante la ecuación de Bellman (Watkins & Dayan, 1992):</p>
          <div class="alert alert-info text-center" style="font-family: monospace; font-size: 1.1em;">
            Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]
          </div>
          <p><strong>Donde:</strong></p>
          <ul>
            <li><strong>α</strong> es la tasa de aprendizaje (learning rate)</li>
            <li><strong>γ</strong> es el factor de descuento (discount factor)</li>
            <li><strong>r</strong> es la recompensa recibida</li>
            <li><strong>s'</strong> es el siguiente estado</li>
          </ul>
        </div>
      </div>
    </div>
    <div class="accordion-item card mb-3">
      <h2 class="accordion-header" id="headingFour">
        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFour">
          Exploración vs. Explotación
        </button>
      </h2>
      <div id="collapseFour" class="accordion-collapse collapse" data-bs-parent="#accordionConceptos">
        <div class="accordion-body">
          <p>Un desafío clave en RL es el balance entre <strong>exploración</strong> (probar nuevas acciones) y <strong>explotación</strong> (usar el conocimiento actual). La estrategia epsilon-greedy es comúnmente utilizada para este propósito (Tokic, 2010).</p>
          <p><strong>Estrategia Epsilon-Greedy:</strong></p>
          <ul>
            <li>Con probabilidad ε, el agente elige una acción aleatoria (exploración)</li>
            <li>Con probabilidad (1-ε), el agente elige la mejor acción conocida (explotación)</li>
            <li>ε generalmente decrece con el tiempo para favorecer la explotación a medida que el agente aprende</li>
          </ul>
        </div>
      </div>
    </div>
    <div class="accordion-item card mb-3">
      <h2 class="accordion-header" id="headingSix">
        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSix">
          Referencias APA
        </button>
      </h2>
      <div id="collapseSix" class="accordion-collapse collapse" data-bs-parent="#accordionConceptos">
        <div class="accordion-body">
          <div class="referencias">
            <p>Sutton, R. S., & Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (2nd ed.). MIT Press.</p>
            <p>Watkins, C. J., & Dayan, P. (1992). Q-learning. <em>Machine Learning, 8</em>(3-4), 279-292. https://doi.org/10.1007/BF00992698</p>
            <p>Tokic, M. (2010). Adaptive ε-greedy exploration in reinforcement learning based on value differences. In <em>Annual Conference on Artificial Intelligence</em> (pp. 203-210). Springer.</p>
            <p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level control through deep reinforcement learning. <em>Nature, 518</em>(7540), 529-533.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div style="text-align: center; margin-top: 30px;">
    <a href="/RLPractico" class="btn btn-primary btn-lg">Ir al Caso Práctico →</a>
  </div>
</div>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
